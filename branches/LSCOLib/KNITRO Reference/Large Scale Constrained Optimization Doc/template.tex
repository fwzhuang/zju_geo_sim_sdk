\documentclass[annual]{acmsiggraph}
\TOGonlineid{45678}
\TOGvolume{0}
\TOGnumber{0}
\TOGarticleDOI{1111111.2222222}
\TOGprojectURL{}
\TOGvideoURL{}
\TOGdataURL{}
\TOGcodeURL{}

\usepackage{mathrsfs}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsthm}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\usepackage{prettyref}
\newrefformat{fig}{Fig.~\ref{#1}}
\newrefformat{par}{section~\ref{#1}}
\newrefformat{sec}{section~\ref{#1}}
\newrefformat{sub}{section~\ref{#1}}
\newrefformat{table}{table~\ref{#1}}
\newrefformat{alg}{algorithm~\ref{#1}}
\newrefformat{Def}{definition~\ref{#1}}
\newrefformat{Thm}{theorem~\ref{#1}}
\newrefformat{step}{step~\ref{#1}}
\newrefformat{eq}{equation~\eqref{#1}}
\newrefformat{pb}{problem~\eqref{#1}}
\def\Eqref Eq:#1:{\eqref{eq:#1}}
\newrefformat{Eq}{Equation~\Eqref#1:}

\newcommand{\E}[1]{\mathbf{#1}}
\newcommand{\FPP}[2]{\frac{\partial{#1}}{\partial{#2}}}
\newcommand{\FDD}[2]{\frac{d{#1}}{d{#2}}}
\newcommand{\TWOC}[2]{\left(\begin{array}{c}#1 \\ #2\end{array}\right)}
\newcommand{\TWOR}[2]{\left(\setlength{\arraycolsep}{1pt}\begin{array}{cc}{#1} & {#2}\end{array}\right)}
\newcommand{\TWORC}[4]{\left(\begin{array}{cc}#1 & #2 \\ #3 & #4\end{array}\right)}
\newcommand{\THREEC}[3]{\left(\begin{array}{c}#1 \\ #2 \\ #3\end{array}\right)}
\newcommand{\THREER}[3]{\left(\setlength{\arraycolsep}{1pt}\begin{array}{ccc}{#1}^T & {#2}^T & {#3}^T\end{array}\right)^T}
\newcommand{\FOURC}[4]{\left(\begin{array}{c}#1 \\ #2 \\ #3 \\ #4\end{array}\right)}
\newcommand{\FOURR}[4]{\left(\setlength{\arraycolsep}{1pt}\begin{array}{cccc}{#1}^T & {#2}^T & {#3}^T & {#4}^T\end{array}\right)^T}
\newcommand{\SIXC}[6]{\left(\begin{array}{c}#1 \\ #2 \\ #3 \\ #4 \\ #5 \\ #6\end{array}\right)}
\newcommand{\SIXR}[6]{\left(\setlength{\arraycolsep}{1pt}\begin{array}{cccc}{#1}^T & {#2}^T & {#3}^T & {#4}^T {#5}^T {#6}^T\end{array}\right)^T}
\newcommand{\MTT}[4]{\left(\begin{array}{cc}#1 & #2 \\ #3 & #4\end{array}\right)}
\newtheorem{myDef}{Definition}
\newtheorem{myTheo}{Theorem}

\title{Large Scaled Constrained Optimization}
\author{Zherong Pan}
\pdfauthor{Zherong Pan}

\begin{document}
\maketitle

\section{General Algorithm}
We implement algorithm \cite{byrd2000trust}, this report is intended to clarify the principles for Tengfei Jiang and list necessary algorithms for implementation convenience. 

Our presentation starts from some helper functions and then move on to main algorithms. Lies in core of \cite{byrd2000trust} is a trust region solution routines with linear constraint, so we analyse these first. Then we present the core algorithm. Finally we list some parameter choice routines.

\section{Trust Region Solver}
\cite{byrd2000trust} requires solving first a normal equation problem and then a linear constrained trust region problem. Both problems are in trust region form, the first one is positive semi-definite with no constraints, the second one is possibly indefinite with linear constraints.

We implement three solvers for these problem: double dogleg, two subspace minimization, the Steihaug-CG. We describe these solvers one by one. In all these methods, we consider the problem:
\begin{subequations}
\label{pb:TR_PROB}
\begin{align}
\E{min}\text{ }&\E{g}^T\E{p}+\frac{1}{2}\E{p}^T\E{H}\E{p}	\\
\E{s.t.}\text{ }&\E{A}\E{p}=\E{0}	\\
\text{ }&\|\E{p}\|\leq\Delta,
\end{align}
\end{subequations}
where $\E{H}^{n\times n}$ is symmetric.

\subsection{The Double Dogleg Method}
This method is suitable of solving medium sized version of \prettyref{pb:TR_PROB}, if $\E{H}|\mathcal{K}(\E{A})$ is positive semi-definite. Let $S^{n\times k}$ be a normalized orthogonal basis of $\mathcal{K}(\E{A})$, we first transform \prettyref{pb:TR_PROB} using $\E{p}=\E{S}\E{u}$ to get:
\begin{subequations}
\label{pb:TR_PROB_R}
\begin{align}
\E{min}\text{ }&\tilde{\E{g}}^T\E{u}+\frac{1}{2}\E{u}^T\tilde{\E{H}}\E{u}	\\
\E{s.t.}\text{ }&\|\E{u}\|_{\E{S}^T\E{S}}\leq\Delta,
\end{align}
\end{subequations}
where $\tilde{\E{g}}=\E{S}^T\E{g}$ and $\tilde{\E{H}}=\E{H}|\mathcal{K}(\E{A})=\E{S}^T\E{H}\E{S}$. We see that $\tilde{\E{H}}$ is also positive semi-definite. Now we define Cauchy Point for \prettyref{pb:TR_PROB_R}:
\begin{eqnarray*}
\E{argmin}_{t^*}&&\tilde{\E{g}}^T\tilde{\E{g}}t+\frac{1}{2}\tilde{\E{g}}^T\tilde{\E{H}}\tilde{\E{g}}t^2	\\
\E{s.t.}&&\|\tilde{\E{g}}t\|_{\E{S}^T\E{S}}\leq\Delta	\\
&&\E{u}^C=\tilde{\E{g}}t^*.
\end{eqnarray*}
Moreover, we define Newton Point for \prettyref{pb:TR_PROB_R}:
\begin{eqnarray*}
\E{u}^N=-\tilde{\E{H}}^\dagger\tilde{\E{g}}.
\end{eqnarray*}
Double Dogleg search for the approximate solution along curve:
\begin{equation*}
\E{u}(\tau)=
\begin{cases}
   \E{u}^C\tau & \text{if } 0 \leq \tau \leq 1 \\
   \E{u}^C+\frac{\tau-1}{\gamma}(\gamma\E{u}^N-\E{u}^C) & \text{if } 1 < \tau \leq \gamma+1	\\
   \E{u}^N(\tau-1) & \text{if } \gamma+1 < \tau \leq 2.
\end{cases}
\end{equation*}
We finally recover the solution using $\E{p}=\E{S}\E{u}$. 

In practice, we don't find $\E{S}$, which requires a rank revealing decomposition. We find $\E{S}\E{S}^T\E{g}=\E{g}^P$ and $\E{S}\E{u}^N=\E{p}^N$ directly. To do this we need following important results:
\begin{myTheo}
\label{Thm:THEO}
If $\tilde{\E{g}} \in \mathcal{I}(\E{H})$ and $\E{H}^\dagger$ is obtained by calculating SVD of $\E{H}$ and inverse each nonzero element of $\Sigma$, then there exists $\E{\lambda}_1$ and $\E{\lambda}_2$ such that:
\begin{eqnarray*}
\MTT{\E{H}}{\E{A}^T}{\E{A}}{\E{0}}\TWOC{\E{p}^N}{\E{\lambda}_1}=\TWOC{-\E{g}}{0}	\\
\MTT{\E{I}}{\E{A}^T}{\E{A}}{\E{0}}\TWOC{\E{g}^P}{\E{\lambda}_2}=\TWOC{\E{g}}{0}.
\end{eqnarray*}
\begin{proof}
For the first equation, we have:
\begin{eqnarray*}
\E{S}\E{S}^T(\E{H}\E{p}^N+\E{g})&=&\E{S}(\E{S}^T\E{H}\E{p}^N+\E{S}^T\E{g})	\\
&=&\E{S}(\E{S}^T\E{H}(-\E{S}\tilde{\E{H}}^\dagger\tilde{\E{g}})+\tilde{\E{g}})	\\
&=&\E{S}(-\tilde{\E{H}}\tilde{\E{H}}^\dagger\tilde{\E{g}}+\tilde{\E{g}})=0.
\end{eqnarray*}
But by the fact that $\mathcal{K}(\E{A})\perp\mathcal{I}(\E{A}^T)$ and $\E{Proj}(\mathcal{K}(\E{A}))=\E{S}\E{S}^T$, we know that $\E{H}\E{p}^N+\E{g} \in \mathcal{I}(\E{A}^T)$, i.e. $\E{H}\E{p}^N-\E{g}=-\E{A}^T\lambda_1$.

For the second equation, we know that $\E{g}^P=\E{S}\E{S}^T\E{g}$, we also have $\E{S}^T(\E{g}^P-\E{g})=\E{0}$, so that we have $\E{g}^P-\E{g}=-\E{A}^T\lambda_2$ for the same reason.
\end{proof}
\end{myTheo}

\subsection{The Two Subspace Minimization Method}
This method is suitable of solving medium sized version of \prettyref{pb:TR_PROB} as the double dogleg method, but allows $\E{H}|\mathcal{K}(\E{A})$ to be arbitrary. The only difference of this method with the double dogleg scheme is that, instead of searching along the dogleg shaped curve, we search in the two dimensional subspace spanned by $\{\E{g}^P,\E{g}^N$\}. Let's find out how to do this, we first get a set of unit orthogonal basis $\text{span}(\E{S})=\text{span}(\E{g}^P,\E{g}^N)$, then reduce \prettyref{pb:TR_PROB} using $\E{p}=\E{S}\E{u}$:
\begin{subequations}
\label{pb:TR_PROB_R}
\begin{align}
\E{min}\text{ }&\tilde{\E{g}}^T\E{u}+\frac{1}{2}\E{u}^T\tilde{\E{H}}\E{u}	\\
\E{s.t.}\text{ }&\|\E{u}\|\leq\Delta,
\end{align}
\end{subequations}
where tilde symbols has same meanings as above but note $\E{S}$ is different. We first solve for $-\tilde{\E{H}}\tilde{\E{g}}$, if the solution is in trust region, exit. Otherwise, we parameterize $\E{u}=(\alpha,\sqrt{\Delta^2-\alpha^2})$ and substitute to get reduced objective function:
\begin{eqnarray*}
\tilde{\E{g}}_1\alpha+\tilde{\E{g}}_2\sqrt{\Delta^2-\alpha^2}+\frac{1}{2}\alpha^2\tilde{\E{H}}_{11}+	\\
\frac{1}{2}(\Delta^2-\alpha^2)\tilde{\E{H}}_{22}+\tilde{\E{H}}_{12}\alpha\sqrt{\Delta^2-\alpha^2}.
\end{eqnarray*}
The derivative of this function is:
\begin{eqnarray*}
\frac{1}{\sqrt{\Delta^2-\alpha^2}}(-\alpha\sqrt{\Delta^2-\alpha^2}\tilde{\E{H}}_{22}+(\Delta^2-\alpha^2)\tilde{\E{H}}_{12}-\alpha^2\tilde{\E{H}}_{21}+	\\
\alpha\sqrt{\Delta^2-\alpha^2}\tilde{\E{H}}_{11}-\alpha\tilde{\E{g}}_2+\sqrt{\Delta^2-\alpha^2}\tilde{\E{g}}_1).
\end{eqnarray*}
Square the numerator to get the quadric equation and solve it analytically (Code can be found in a conventional continuous collision detection lib, don't do it yourself). Then, find the solution corresponding to minimal objective value and accept it.

\subsection{The Steihaug-CG Method}
This method is suitable of solving large scaled version of \prettyref{pb:TR_PROB} allowing $\E{H}|\mathcal{K}(\E{A})$ to be arbitrary. To implement an efficient version of this method, we have to take into account of several modifications to standard conjugate gradient:
\begin{enumerate}
\item The linear constraint.
\item The norm of trust region, in our case not a problem.
\item Appropriate preconditioner.
\item Termination condition.
\end{enumerate}

We begin our analysis on a conventional CG method for solving $\E{H}\E{x}=-\E{g}$ where $\E{H}$ is positive semi-definite. The idea is to search for solution $\E{x}_k$ in $\E{span}(\E{d}_0,\cdots,\E{d}_{k-1})$ at the end of iteration $k$, such that $\E{span}(\E{d}_0,\cdots,\E{d}_{k-1})=\E{span}(\E{r}_0,\E{H}\E{r}_0,\cdots,\E{H}^{k-1}\E{r}_0)$. Then at iteration $k$, expand this basis using:
\begin{eqnarray*}
\E{r}_k&=&\E{g}+\E{H}\E{x}	\\
\E{d}_k&=&\E{r}_k+\beta\E{d}_{k-1}	\\
\beta&=&-\frac{\E{r}_k^T\E{H}\E{d}_{k-1}}{\E{d}_{k-1}^T\E{H}\E{d}_{k-1}}.
\end{eqnarray*}
We note that if $\E{d}_k$ is close to zero, then we know that $\E{r}_k$ is almost in $\E{span}(\E{d}_0,\cdots,\E{d}_{k-1})$, but $x_k$ is minimized in this subspace, thus $\E{r}_k$ must also be close to zero and the procedure can terminate. The complete algorithm is \prettyref{alg:CG}.
\begin{algorithm}[h]
\caption{CG Algorithm}
\label{alg:CG}
\begin{algorithmic}
\REQUIRE $\E{x}$, $\E{r}=\E{g}+\E{H}\E{x}$ and $\E{d}=\E{r}$
\ENSURE $\E{x}^*$
\WHILE{$\|\E{r}\|$ not small}
\STATE $\alpha=\frac{\E{r}^T\E{d}}{\E{d}^T\E{H}\E{d}}$
\STATE $\E{x}=\E{x}-\alpha\E{d}$
\STATE $\E{r}=\E{r}-\alpha\E{H}\E{d}$
\STATE $\beta=-\frac{\E{r}^T\E{H}\E{d}}{\E{d}^T\E{H}\E{d}}$
\STATE $\E{d}=\E{r}+\beta\E{d}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

Our first modification comes from the fact that we are now using $\E{H}|\mathcal{K}(\E{A})$ instead of $\E{H}$, but forming a basis for $\mathcal{K}(\E{A})$ is too expansive. But there is another easier alternative, we project $\E{r}_k$ to $\mathcal{K}(\E{A})$ using the second equation of \prettyref{Thm:THEO} to get:
\begin{eqnarray*}
\E{d}_k&=&\tilde{\E{r}}_k+\beta\E{d}_{k-1}	\\
\MTT{\E{I}}{\E{A}^T}{\E{A}}{\E{0}}\TWOC{\tilde{\E{r}}_k}{\E{\lambda}}&=&\TWOC{\E{r}_k}{0}.
\end{eqnarray*}
To validate this approach, we first note that by doing so, $\E{span}(\E{d}_0,\cdots,\E{d}_{k-1}) \in \mathcal{K}(\E{A})$, so the constraint is always satisfied as long as the initial value is properly chosen. Moreover, we note that if $\E{d}_k$ is close to zero, then $\tilde{\E{r}}_k=\E{S}\E{S}^T\E{r}_k$ is in $\E{span}(\E{d}_0,\cdots,\E{d}_{k-1})$. But $\E{x}$ is already minimized in this subspace, so we have $\tilde{\E{r}}_k$ is also close to zero. In that case, we have $\tilde{\E{r}}=\E{S}\E{S}^T(\E{H}\E{x}+\E{g})=0$, and by using the fact that $\mathcal{K}(\E{A})\perp\mathcal{I}(\E{A}^T)$, we have $\E{H}\E{x}+\E{g}=-\E{A}^T\lambda$. Putting these results together, we have that when the algorithm converges, the first equation of \prettyref{Thm:THEO} is satisfied and we are essentially solving the corresponding KKT system. This upgraded algorithm is \prettyref{alg:KKTCG}, note that we have to preserve $\tilde{\E{r}}$ besides $\E{r}$ this time.
\begin{algorithm}[h]
\caption{KKT-CG Algorithm}
\label{alg:KKTCG}
\begin{algorithmic}
\REQUIRE $\E{x}$, $\E{r}=\E{g}+\E{H}\E{x}$, $\tilde{\E{r}}=\E{S}\E{S}^T\E{r}$ and $\E{d}=\tilde{\E{r}}$
\ENSURE $\E{x}^*$
\WHILE{$\|\tilde{\E{r}}\|$ not small}
\STATE $\alpha=\frac{\tilde{\E{r}}^T\E{d}}{\E{d}^T\E{H}\E{d}}=\frac{\E{r}^T\E{d}}{\E{d}^T\E{H}\E{d}}$
\STATE $\E{x}=\E{x}-\alpha\E{d}$
\STATE $\E{r}=\E{r}-\alpha\E{H}\E{d}$
\STATE $\tilde{\E{r}}=\E{S}\E{S}^T\E{r}$
\STATE $\beta=-\frac{\tilde{\E{r}}^T\E{H}\E{d}}{\E{d}^T\E{H}\E{d}}$
\STATE $\E{d}=\tilde{\E{r}}+\beta\E{d}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

Our next modification is to precondition KKT-CG algorithm on both side. This is simple, we just solve the modified linear system:
\begin{eqnarray*}
\E{M}&=&\E{L}\E{L}^T	\\
\E{L}^{-1}\E{H}\E{L}^{-T}\E{y}&=&-\E{L}^{-1}\E{g}	\\
\E{x}&=&\E{L}^{-T}\E{y}.
\end{eqnarray*}
But we don't derive it this way, because we will soon find that the requirement of subspace and the preconditioner is interwoven. Instead, we note that in \prettyref{alg:CG}, if we don't generate new $\E{d}$ using $\E{r}$ but we use $\E{H}^{-1}\E{r}$, then in next iteration the exact solution is attained. Therefore, we try to approximate $\E{H}^{-1}$ using $\E{M}^{-1}$ which is usually easy to solve and generate new $\E{d}$ using $\E{r}=\E{M}^{-1}\E{r}$. However, this $\E{r}$ must also be in $\mathcal{K}(\E{A})$. Combine both considerations, we generate $\E{d}$ by solving the following equation:
\begin{eqnarray*}
\MTT{\E{M}}{\E{A}^T}{\E{A}}{\E{0}}\TWOC{\tilde{\E{r}}}{\E{\lambda}}&=&\TWOC{\E{r}}{0},
\end{eqnarray*}
yielding \prettyref{alg:KKTPCG}.
\begin{algorithm}[h]
\caption{KKT-PCG Algorithm}
\label{alg:KKTPCG}
\begin{algorithmic}
\REQUIRE $\E{x}$, $\E{r}=\E{g}+\E{H}\E{x}$, $\tilde{\E{r}}=\E{S}\E{S}^T\E{r}$ and $\E{d}=\tilde{\E{r}}$
\ENSURE $\E{x}^*$
\WHILE{$\|\tilde{\E{r}}\|$ not small}
\STATE $\alpha=\frac{\tilde{\E{r}}^T\E{d}}{\E{d}^T\E{H}\E{d}}=\frac{\E{r}^T\E{d}}{\E{d}^T\E{H}\E{d}}$
\STATE $\E{x}=\E{x}-\alpha\E{d}$
\STATE $\E{r}=\E{r}-\alpha\E{H}\E{d}$
\STATE Solving $\MTT{\E{M}}{\E{A}^T}{\E{A}}{\E{0}}\TWOC{\tilde{\E{r}}}{\E{\lambda}}=\TWOC{\E{r}}{0}$
\STATE $\beta=-\frac{\tilde{\E{r}}^T\E{H}\E{d}}{\E{d}^T\E{H}\E{d}}$
\STATE $\E{d}=\tilde{\E{r}}+\beta\E{d}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

Our final modification is to use Steihaug's approach \cite{steihaug1983conjugate} to solve the trust-region problem using above routine. This can be done by adding additional termination conditions carefully. We have to handle two extra conditions in addition to $\|\tilde{\E{r}}\|$ close to zero. The first one is that $\E{x}-\alpha\E{d}$ is outside region, in this case we clamp on boundary. The second one is that we encounter negative curvature, in this case we go along this direction to boundary. Both are very easy to understand. At last, we have reached our final version \prettyref{alg:TRKKTPCG}.
\begin{algorithm}[h]
\caption{TrustRegion-KKT-PCG Algorithm}
\label{alg:TRKKTPCG}
\begin{algorithmic}
\REQUIRE $\E{x}$, $\E{r}=\E{g}+\E{H}\E{x}$, $\tilde{\E{r}}=\E{S}\E{S}^T\E{r}$ and $\E{d}=\tilde{\E{r}}$
\ENSURE $\E{x}^*$
\WHILE{true}
\STATE $\alpha=\frac{\tilde{\E{r}}^T\E{d}}{\E{d}^T\E{H}\E{d}}=\frac{\E{r}^T\E{d}}{\E{d}^T\E{H}\E{d}}$
\IF {$\E{d}^T\E{H}\E{d}\leq0$}
\STATE Set $\E{x}=\E{x}+\tau\E{d}$ such that $\|\E{x}\|=\Delta$
\STATE exit
\ENDIF
\STATE $\E{x}=\E{x}-\alpha\E{d}$
\IF{$\|\E{x}\|>\Delta$}
\STATE Clamp $\E{x}$ to boundary
\STATE exit
\ENDIF
\STATE $\E{r}=\E{r}-\alpha\E{H}\E{d}$
\IF{$\|\E{r}\|$ small enough}
\STATE exit
\ENDIF
\STATE Solving $\MTT{\E{M}}{\E{A}^T}{\E{A}}{\E{0}}\TWOC{\tilde{\E{r}}}{\E{\lambda}}=\TWOC{\E{r}}{0}$
\STATE $\beta=-\frac{\tilde{\E{r}}^T\E{H}\E{d}}{\E{d}^T\E{H}\E{d}}$
\STATE $\E{d}=\tilde{\E{r}}+\beta\E{d}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}
The validity of this algorithm depends on the fact that as long as $\E{d}^T\E{H}\E{d}>0$, $\E{H}|\E{span}(\E{d}_0,\cdots,\E{d}_{k-1})$ is positive semi-definite. Therefore, CG algorithm performs well. There's one problem when using constraint preconditioner, numerical drifting may cause $\E{A}\E{d}\neq\E{0}$. To handle this degeneracy, we project to get $\E{A}(\E{x}^*-\E{x})=\E{0}$ again after the whole algorithm.

\section{Large Scale Nonlinear Optimization}
Here we consider the core algorithms implemented in our solver. These are aimed at large scale non-convex nonlinear optimization problem. Our first solver is the Primal/Interior-Point/Trust-Region algorithm \cite{nocedalnumerical}. It's designed for the following problem:
\begin{subequations}
\label{pb:ORIGIN}
\begin{align}
\E{min}\text{ }&f(\E{x})	\\
\E{s.t.}\text{ }&\E{c}_E(\E{x})=\E{0}	\\
\text{ }&\E{c}_I(\E{x}) \geq \E{0}.
\end{align}
\end{subequations}
The KKT condition for this problem is, by introducing equality lagrangian multiplier $\E{y}$ and inequality multiplier $\E{z}$, as follows:
\begin{eqnarray*}
\nabla f-\nabla\E{c}_E^T\E{y}-\nabla\E{c}_I^T\E{z}=\E{0}	\\
\E{c}_E(\E{x})=\E{0}	\\
\E{c}_I(\E{x}) \geq \E{0}	\\
\E{z} \geq \E{0}	\\
\E{c}_I(\E{x})^T\E{z}=\E{0},
\end{eqnarray*}
we first introduce a slack variable $\E{s}=\E{c}_I(\E{x})$ to get an equivalent KKT condition:
\begin{eqnarray*}
\nabla f-\nabla\E{c}_E^T\E{y}-\nabla\E{c}_I^T\E{z}=\E{0}	\\
\E{c}_E(\E{x})=\E{0}	\\
\E{c}_I(\E{x})-\E{s}=\E{0}	\\
\E{z} \geq \E{0} \text{ , } \E{s} \geq \E{0}	\\
\E{s}^T\E{z}=\E{0}.
\end{eqnarray*}
The above two KKT system are equivalent and of course suffer from same problem of combinatorial explosion. One practical method to alleviate this issue is to slightly perturb the KKT system, allowing $\E{s}^T\E{z} > 0$:
\begin{eqnarray*}
\nabla f-\nabla\E{c}_E^T\E{y}-\nabla\E{c}_I^T\E{z}=\E{0}	\\
\E{c}_E(\E{x})=\E{0}	\\
\E{c}_I(\E{x})-\E{s}=\E{0}	\\
\E{z} \geq \E{0} \text{ , } \E{s} \geq \E{0}	\\
\E{S} \E{z}=\mu \E{e}.
\end{eqnarray*}
By gradually decrease $\mu$, we approach the original KKT system. Another way to derive this perturbed KKT system is by using log-barrier function:
\begin{eqnarray*}
\E{min}&&f(\E{x})-\mu\sum_i\E{log}\E{s}_i	\\
\E{s.t.}&&\E{c}_E(\E{x})=\E{0}	\\
&&\E{c}_I(\E{x})=\E{s},
\end{eqnarray*}
whose KKT system is exactly the same as above if we move $\E{S}$ to the RHS:
\begin{subequations}
\label{pb:BARRIER_KKT}
\begin{align}
\nabla f-\nabla\E{c}_E^T\E{y}-\nabla\E{c}_I^T\E{z}=\E{0}	\\
\E{c}_E(\E{x})=\E{0}	\\
\E{c}_I(\E{x})-\E{s}=\E{0}	\\
\E{z} \geq \E{0} \text{ , } \E{s} \geq \E{0}	\\
\E{z}=\mu \E{S}^{-1} \E{e}.
\end{align}
\end{subequations}
Therefore, barrier method and interior-point/trust-region method can be used interchanged. Trust region solver works in this manner, in each iteration, it fix $\mu$ and solve this perturbed system using trust-region method. The is the general framework \prettyref{alg:MAIN_ALG}.
\begin{algorithm}[h]
\caption{Primal/Interior-Point/Trust-Region Algorithm}
\label{alg:MAIN_ALG}
\begin{algorithmic}
\WHILE{the original \prettyref{pb:ORIGIN} is not solved}
\STATE solve \prettyref{pb:BARRIER_KKT} using trust-region inner iteration
\STATE decrease $\mu$ using Fiacco-McCormick law
\ENDWHILE
\end{algorithmic}
\end{algorithm}
In the next several subsections, we refine these steps. We first show how the trust-region subproblem is formulated and solved. Then we show how the various parameters are initialized and updated.

\subsection{The Trust-Region Subproblem}
The trust region is based on lagrangian multiplier applied on \prettyref{pb:BARRIER_KKT}, we call our method Primal method because we estimate the dual parameters $\E{y}$ and $\E{z}$ in a separate pass. Then fix $\E{y}^*$ and $\E{z}^*$ and solve the following primal problem to update $\E{x}$ and $\E{s}$ to get $\E{x}+\delta\E{x}$ and $\E{s}+\delta\E{s}$:
\begin{eqnarray*}
\E{solve}_{\delta\E{x}, \delta\E{s}}&&-(\nabla f-\nabla\E{c}_E^T\E{y}^*-\nabla\E{c}_I^T\E{z}^*)=	\\
&&(\nabla^2 f-\E{y}^{*T}\nabla\E{c}_E^2-\E{z}^{*T}\nabla\E{c}_I^2)\delta\E{x}	\\
&&\E{s}+\delta\E{s}=\mu {\E{Z}^*}^{-1} \E{e}	\\
\E{s.t.}&&\E{c}_E(\E{x})+\nabla\E{c}_E\delta\E{x}=\E{0}	\\
&&\E{c}_I(\E{x})+\nabla\E{c}_I\delta\E{x}-\E{s}-\delta\E{s}=\E{0}	\\
&&\E{s} \geq \E{0},
\end{eqnarray*}
if we rewrites $\mathcal{L}=\nabla f-\E{c}_E^T\E{y}^*-(\E{c}_I-\E{s})^T\E{z}^*$ then the corresponding quadratic programming problem is:
\begin{eqnarray*}
\E{min}_{\delta\E{x}, \delta\E{s}}&&\nabla f+\frac{1}{2}\delta\E{x}^T\nabla^2_{\E{x}\E{x}}\mathcal{L}\delta\E{x}-\mu\E{e}\E{S}^{-1}\delta\E{s}+\frac{1}{2}\delta\E{s}^T\E{S}^{-1}\E{Z}^*\delta\E{s}	\\
\E{s.t.}&&\E{c}_E(\E{x})+\nabla\E{c}_E\delta\E{x}=\E{0}	\\
&&\E{c}_I(\E{x})+\nabla\E{c}_I\delta\E{x}-\E{s}-\delta\E{s}=\E{0}	\\
&&\E{s}+\delta\E{s} \geq \E{0}	\\
&&\|\TWOC{\delta\E{x}}{\E{S}^{-1}\delta\E{s}}\|_2\leq\Delta.
\end{eqnarray*}
Where we have introduced the trust-region constraint, the system of equations are solved using Trust-Region-KKT-PCG algorithm and the constraint $\E{s}+\delta\E{s} \geq \E{0}$ is just applied by post-projection. This original form needs several further modifications. First, to avoid $\E{s}$ from being to close to zero, we relax above constraint into $\E{s}+\delta\E{s} \geq (1-\tau) \E{s}$. Moreover, the above constraints may be inconsisitent (so solution), to handle this, we relax the first two constriant into $\E{c}_E(\E{x})+\nabla\E{c}_E\delta\E{x}=\E{r}_E$ and $\E{c}_I(\E{x})+\nabla\E{c}_I\delta\E{x}-\E{s}-\delta\E{s}=\E{r}_I$. The final form reads:
\begin{eqnarray*}
\E{min}&&\nabla f^T\delta\E{x}+\frac{1}{2}\delta\E{x}^T\nabla^2_{\E{x}\E{x}}\mathcal{L}\delta\E{x}-\mu\E{e}\E{S}^{-1}\delta\E{s}+\frac{1}{2}\delta\E{s}^T\E{S}^{-1}\E{Z}^*\delta\E{s}	\\
\E{s.t.}&&\E{c}_E(\E{x})+\nabla\E{c}_E\delta\E{x}=\E{r}_E\\
&&\E{c}_I(\E{x})+\nabla\E{c}_I\delta\E{x}-\E{s}-\delta\E{s}=\E{r}_I	\\
&&\delta\E{s} \geq -\tau\E{s}	\\
&&\|\TWOC{\delta\E{x}}{\E{S}^{-1}\delta\E{s}}\|_2\leq\Delta.
\end{eqnarray*}

Now there is one remaining problems to be solved: how to determine $\E{r}_E$ and $\E{r}_I$. To do this, we first solve the so-called normal equation:
\begin{eqnarray*}
\E{min}&&\|\E{c}_E(\E{x})+\nabla\E{c}_E\E{v}_\E{x}\|^2+\|\E{c}_I(\E{x})+\nabla\E{c}_I\E{v}_\E{x}-\E{s}-\E{v}_\E{s}\|^2	\\
\E{s.t.}&&\|\TWOC{\E{v}_\E{x}}{\E{S}^{-1}\E{v}_\E{s}}\|_2\leq\zeta\Delta,
\end{eqnarray*}
where we currently set $\zeta=0.8$. We then take $\E{c}_E(\E{x})+\nabla\E{c}_E\E{v}_\E{x}=\E{r}_E$ and $\E{c}_I(\E{x})+\nabla\E{c}_I\E{v}_\E{x}-\E{s}-\E{v}_\E{s}=\E{r}_I$. Besides, this preprocessing also provides initial value for the projection conjugate gradient solver. Note that before solving any of the above two equation, we first transform using $\delta\tilde{\E{s}}=\E{S}^{-1}\delta\E{s}$ and $\tilde{\E{v}}_\E{s}=\E{S}^{-1}\E{v}_\E{s}$ into a normal trust-region problem. We give their final form for implementation convenience:
\begin{subequations}
\label{pb:NORMAL}
\begin{align}
\E{solve}_{\E{v}_\E{x}, \tilde{\E{v}}_\E{s}}\text{ }&\E{A}^T\E{A}\TWOC{\E{v}_\E{x}}{\tilde{\E{v}}_\E{s}}=\E{A}^T\TWOC{-\E{c}_E(\E{x})}{-\E{c}_I(\E{x})+\E{s}}	\\
\E{s.t.}\text{ }&\|\TWOC{\E{v}_\E{x}}{\tilde{\E{v}}_\E{s}}\|_2\leq\zeta\Delta,
\end{align}
\end{subequations}
where we have $\E{A}=\TWORC{\nabla \E{c}_E}{\E{0}}{\nabla\E{c}_I}{-\E{S}}$ and
\begin{subequations}
\label{pb:QP}
\begin{align}
\E{solve}_{\delta\E{x}, \delta\tilde{\E{s}}}\text{ }&\TWORC{\nabla^2_{\E{x}\E{x}}\mathcal{L}}{\E{0}}{\E{0}}{\E{Z}^*\E{S}}\TWOC{\delta\E{x}}{\delta\tilde{\E{s}}}=\TWOC{-\nabla f}{\mu\E{e}}	\\
\E{s.t.}\text{ }&\E{A}\TWOC{\delta\E{x}}{\delta\tilde{\E{s}}}=\TWOC{\nabla \E{c}_E\E{v}_\E{x}}{\nabla\E{c}_I\E{v}_x-\E{v}_s}=\E{A}\TWOC{\E{v}_\E{x}}{\tilde{\E{v}}_\E{s}}	\\
\text{ }&\E{S}\delta\tilde{\E{s}} \geq -\tau\E{s}	\\
\text{ }&\|\TWOC{\delta\E{x}}{\delta\tilde{\E{s}}}\|_2\leq\Delta.
\end{align}
\end{subequations}
At this point, we know how to solve the trust-region subproblem, but to define this problem, we still have to estimate the lagrangian multiplier $\E{y}^*$ and $\E{z}^*$, as all the other Primal solvers. We do this by solving the KKT system involved in \prettyref{pb:QP} and replace the hessian with identity. Finally, we get the reduced form by gauss-elimination:
\begin{eqnarray}
\label{pb:LAG}
\TWOC{\E{y}}{\E{z}}=(\E{A}\E{A}^T)^{-1}\E{A}\TWOC{\nabla f}{-\mu \E{e}}
\end{eqnarray}
and then project $\E{z}_i=\E{min}(10^{-3},\frac{\mu}{\E{s}_i})$ to ensure $\E{z}>\E{0}$. At last, we can write our trust-region inner iteration \prettyref{alg:INNER_ITER}.
\begin{algorithm}[h]
\caption{Trust-Region Inner Iteration}
\label{alg:INNER_ITER}
\begin{algorithmic}
\WHILE{\prettyref{pb:BARRIER_KKT} is not satisfied to some threshold}
\STATE solve \prettyref{pb:NORMAL} for $(\delta\E{v}_\E{x}, \delta\E{v}_\E{s})$
\STATE solve \prettyref{pb:LAG} for $(\E{y}^*, \E{z}^*)$
\STATE solve \prettyref{pb:QP} for $(\delta\E{x}, \delta\tilde{\E{s}})$
\STATE update $\E{x}=\E{x}+\delta\E{x}$ and $\E{s}=\E{s}+\E{S}\delta\tilde{\E{s}}$
\STATE update trust region radius $\Delta$
\STATE decrease $\mu$ using Adaptive law
\ENDWHILE
\end{algorithmic}
\end{algorithm}

In actual implementation, we usually get $\E{A}$ with very small entries, causing unstable factorization for $\E{A}\E{A}^T$ and $\E{A}^T\E{A}$. To handle this, we perform re-normalize for each row of $\E{A}$ and solve the substituted problem. E.g., for the normal equation, we solve the following system:
\begin{eqnarray*}
\E{solve}_{\E{v}_\E{x}, \tilde{\E{v}}_\E{s}}\text{ }&\E{A}^T\E{P}^2\E{A}\TWOC{\E{v}_\E{x}}{\tilde{\E{v}}_\E{s}}=\E{A}^T\E{P}^T\E{P}\TWOC{-\E{c}_E(\E{x})}{-\E{c}_I(\E{x})+\E{s}}	\\
\E{s.t.}\text{ }&\|\TWOC{\E{v}_\E{x}}{\tilde{\E{v}}_\E{s}}\|_2\leq\zeta\Delta,
\end{eqnarray*}
for the kkt preconditioner in Steihuag's algorithm, we solve:
\begin{eqnarray*}
\MTT{\E{M}}{\E{A}^T\E{P}^T}{\E{P}\E{A}}{\E{0}}\TWOC{\tilde{\E{r}}}{\E{\lambda}}=\TWOC{\E{r}}{0},
\end{eqnarray*}
and finally, for the multiplier, we solve:
\begin{eqnarray*}
\TWOC{\E{y}}{\E{z}}=\E{P}(\E{P}\E{A}\E{A}^T\E{P}^T)^{-1}\E{P}\E{A}\TWOC{\nabla f}{-\mu \E{e}}
\end{eqnarray*}

\subsection{Parameter Tuning and Choice}
In this subsection, we show how our parameters are initialized and updated. Our first parameter is the KKT perturbation level: $\mu$. We have two loops in the algorithm pipeline, $\mu$ can be updated in both inner and outter iterations. We combine two laws for $\mu$ updation, the LOQO law which change $\mu$ in each inner iterations as follows:
\begin{eqnarray*}
\mu&=&\sigma\frac{\E{s}^T\E{z}}{N_I}	\\
\sigma&=&0.1\E{min}(0.05\frac{1-\iota}{\iota},2)^3	\\
\iota&=&\frac{\E{min}_i\{\E{s}_i\E{z}_i\}}{\E{s}^T\E{z}/N_I},
\end{eqnarray*}
we begin with and continue using LOQO as long as the original problem KKT error is reduced by a constant $\kappa=0.95$ at least once every $l_{max}=5$ iterations, i.e. as long as the following condition is satisfied:
\begin{eqnarray*}
\E{err}_k \leq \kappa \E{max}\{\E{err}_{k-1},\cdots,\E{err}_{k-l_{max}}\}.
\end{eqnarray*}
Otherwise, we use Fiacco-McCormick law, which fix $\mu$ in inner iteration and set $\mu=\sigma_0\mu$ in each other iteration where $\sigma_0=0.2$. Until the above condition is true again, we can then switch back to LOQO law. See \cite{nocedal2009adaptive} for more detail.

Another parameter is the trust-region radius: $\Delta$. Our method is based on the following commonly used merit function:
\begin{eqnarray*}
\phi_\nu(\E{x},\E{s})&=&f(\E{x})-\mu\sum_{i=1}^{N_I}\E{log}\E{s}_i	\\
&+&\nu\|\E{c}_E(\E{x})\|_2+\nu\|\E{c}_I(\E{x})-\E{s}\|_2
\end{eqnarray*}
and predicated merit function:
\begin{eqnarray*}
Q_\nu(\delta\E{x},\delta\E{s})&=&\nabla f^T\delta\E{x}-\mu\E{e}^T\E{S}^{-1}\delta\E{s}	\\
&+&\frac{1}{2}(\delta\E{x}^T\nabla^2_{\E{x}\E{x}}\mathcal{L}\delta\E{x}+\delta\E{s}^T\Sigma\delta\E{s})	\\
&+&\nu m(\delta\E{x},\delta\E{s})	\\
m(\delta\E{x},\delta\E{s})&=&\|\TWOC{\nabla\E{c}_E\delta\E{x}+\E{c}_E(\E{x})}{\nabla\E{c}_I\delta\E{x}+\E{c}_I(\E{x})-\E{s}-\delta\E{s}}\|_2.
\end{eqnarray*}
Trust region radius is updated by comparing $ared=\phi_\nu(\E{x},\E{s})-\phi_\nu(\E{x}+\delta\E{x},\E{s}+\delta\E{s})$ and $pred=Q_\nu(\E{0},\E{0})-Q_\nu(\delta\E{x},\delta\E{s})$. If we find that $ared > 0$ and $ared > \eta_0 pred$, we accept the new value $\E{x}=\E{x}+\delta\E{x}$, $\E{s}=\E{s}+\delta\E{s}$ and reject it otherwise. If we find that $ared < 0$ or $ared < \eta_1 pred$, we update $\Delta=\tau_1\Delta$, or if we find that $ared >= 0$ but $ared < \eta_2 pred$, we keep $\Delta$, otherwise we update $\Delta=\tau_2\Delta$. Currently we set $\eta_0=0.1$, $\eta_1=1/3$, $\eta_2=2/3$, $\tau_1=2/3$ and $\tau_2=3/2$. 

In this formula, we have to determine the parameter $\nu$, this is set so that $pred \geq \rho\nu(m(\E{0},\E{0})-m(\delta\E{x},\delta\E{s}))$. If this is satisfied, $\nu$ is left unchanged, otherwise it's increased, currently we set $\rho=0.1$.

Finally, for the initial trust region radius, we just set $\Delta=0.1\|\nabla f\|$ as in LANCELOT. But more robust algorithm exists such as \cite{sartenaer1997automatic}.

\section{Quasi-Newton Hessian Approximation}
We encorporate two equations of Quasi-Newton update. This allow user to provide Hessian free function (L-D-BFGS) or approximate Hessian (L-SR-1). The two algorithms we implement are detailed below. The Quasi-Newton updation is applied on function $\E{f}$ and constraints $\E{c}_E$ and $\E{c}_I$ as well if user required, since $\nabla^2_{\E{x}\E{x}}\mathcal{L}$ needs all of these.

\subsection{L-D-BFGS Update Formula}
We use compact representation for approximate hessian $\E{H}^{BFGS}$:
\begin{eqnarray*}
\E{H}^{BFGS}_{new}&=&\delta\E{I}-\TWOR{\delta\E{S}}{\E{Y}}\TWORC{\delta\E{S}^T\E{S}}{\E{L}}{\E{L}^T}{-\E{D}}^{-1}\TWOC{\delta\E{S}^T}{\E{Y}^T}	\\
\E{L}_{ij}&=&\E{S}_i^T\E{Y}_j\E{ , }i>j	\\
\E{S}_i&=&\E{x}_i-\E{x}_{i-1}	\\
\E{Y}_i&=&\nabla\E{x}_i-\nabla\E{x}_{i-1}	\\
\delta&=&\frac{\E{Y}_{curr}^T\E{Y}_{curr}}{\E{S}_{curr}^T\E{Y}_{curr}}.
\end{eqnarray*}
For robustness, user is allowed to damp the above equation to ensure positive definite $\E{H}^{BFGS}$. We just modify $\E{Y}_i$ as follows:
\begin{eqnarray*}
\E{Y}_i^*&=&\nabla\E{x}_{curr}-\nabla\E{x}_{last}	\\
\E{Y}_i&=&\theta\E{Y}_i^*+(1-\theta)\E{B}\E{S}_{last}	\\
\theta&=&
\begin{cases}
   1 & \text{if cond}	\\
   \frac{0.8\E{S}_{curr}^T\E{H}^{BFGS}_{old}\E{S}_{curr}}{\E{S}_{curr}^T\E{H}^{BFGS}_{old}\E{S}_{curr}-\E{S}_{curr}^T\E{Y}_{curr}} & \text{otherwise}
\end{cases}	\\
\text{cond}&=&\E{S}_{curr}^T\E{Y}_{curr} \geq 0.2\E{S}_{curr}^T\E{H}^{BFGS}_{old}\E{S}_{curr}.
\end{eqnarray*}
Also, if $\E{Y}_i^T\E{S}_i$ is too small, this updation are skipped.

\subsection{L-SR1 Update Formula}
L-D-BFGS has two drawbacks, one is that it is positive definite, which is too restrictive for SQP solver. The other is that user is not allowed to provide any known information to somehow precondition the Quasi-Newton method. While L-SR1 has no such drawbacks, whose compact update formula is as follows:
\begin{eqnarray*}
\E{B}^{SR1}&=&\E{B}_0+(\E{Y}-\E{B}_0\E{S})M^{-1}(\E{Y}-\E{B}_0\E{S})^T	\\
\E{M}&=&\E{D}+\E{L}+\E{L}^T-\E{S}^T\E{B}_0\E{S}.
\end{eqnarray*}
If $|\E{S}_i^T(\E{Y}_i-\E{B}^{SR1}_{old}\E{S}_i)| < r\|\E{S}_i\|\|\E{Y}_i-\E{B}^{SR1}_{old}\E{S}_i\|$ (where $r=10^{-6}$), this updation is skipped.

In both SR1 and BFGS updation, note that the old matrix is formed by excluding the oldest $\E{S}$ and $\E{Y}$ pair from the current matrix.

\bibliographystyle{acmsiggraph}
\bibliography{template}
\end{document}
